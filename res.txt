+ set -eu
+ : true
+ '[' true '=' true ]
+ containerd-entrypoint.sh
+ '[' 1 -eq 0 ]
+ '[' start.sh '!=' start.sh ]
+ '[' start.sh '=' dockerd ]
+ set -- docker-entrypoint.sh start.sh
+ exec docker-entrypoint.sh start.sh
+ set -eu
+ CONTAINER_LOG_LEVEL=info
+ '[' 0 -eq 0 ]
+ containerdConfig=/etc/containerd/config.toml
+ set -- containerd --config /etc/containerd/config.toml --log-level info
+ exec containerd --config /etc/containerd/config.toml --log-level info
+ set -eu
+ '[' start.sh '!=' start.sh ]
+ docker help start.sh
+ '[' -z  ]
+ '[' -S /var/run/docker.sock ]
+ '[' -z  ]
+ id -u
+ XDG_RUNTIME_DIR=/run/user/0
+ '[' -S /run/user/0/docker.sock ]
+ '[' -z  ]
+ _should_tls
+ '[' -n /certs ]
+ '[' -s /certs/client/ca.pem ]
+ '[' -n  ]
+ export 'DOCKER_HOST=tcp://docker:2375'
+ '[' //docker:2375 '!=' tcp://docker:2375 ]
+ '[' -z  ]
+ '[' -z  ]
+ _should_tls
+ '[' -n /certs ]
+ '[' -s /certs/client/ca.pem ]
+ '[' start.sh '=' dockerd ]
+ exec start.sh
+ set -eu
+ preconfig.sh
+ set -eu
+ mkdir -p /etc/containerd
+ containerd config default
[33mWARN[0m[0000] containerd config version `1` has been deprecated and will be removed in containerd v2.0, please switch to version `2`, see https://github.com/containerd/containerd/blob/main/docs/PLUGINS.md#version-header 
[36mINFO[0m[2025-04-24T11:25:25.754407096Z] starting containerd                           [36mrevision[0m=2806fc1057397dbaeefbea0e4e17bddfbd388f38 [36mversion[0m=v1.6.20
[36mINFO[0m[2025-04-24T11:25:25.769493537Z] loading plugin "io.containerd.content.v1.content"...  [36mtype[0m=io.containerd.content.v1
[36mINFO[0m[2025-04-24T11:25:25.770008110Z] loading plugin "io.containerd.snapshotter.v1.aufs"...  [36mtype[0m=io.containerd.snapshotter.v1
+ mkdir -p /kubeadm_install
+ kubeadm config print init-defaults
[36mINFO[0m[2025-04-24T11:25:25.810105541Z] skip loading plugin "io.containerd.snapshotter.v1.aufs"...  [36merror[0m="aufs is not supported (modprobe aufs failed: exit status 1 \"ip: can't find device 'aufs'\\nmodprobe: module aufs not found in modules.dep\\n\"): skip plugin" [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:25.810151146Z] loading plugin "io.containerd.snapshotter.v1.devmapper"...  [36mtype[0m=io.containerd.snapshotter.v1
[33mWARN[0m[2025-04-24T11:25:25.810187639Z] failed to load plugin io.containerd.snapshotter.v1.devmapper  [33merror[0m="devmapper not configured"
[36mINFO[0m[2025-04-24T11:25:25.810202596Z] loading plugin "io.containerd.snapshotter.v1.native"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:25.810356544Z] loading plugin "io.containerd.snapshotter.v1.overlayfs"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:25.810615365Z] loading plugin "io.containerd.snapshotter.v1.zfs"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:25.811138905Z] skip loading plugin "io.containerd.snapshotter.v1.zfs"...  [36merror[0m="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:25.811162019Z] loading plugin "io.containerd.metadata.v1.bolt"...  [36mtype[0m=io.containerd.metadata.v1
[33mWARN[0m[2025-04-24T11:25:25.811235045Z] could not use snapshotter devmapper in metadata plugin  [33merror[0m="devmapper not configured"
[36mINFO[0m[2025-04-24T11:25:25.811250040Z] metadata content store policy set             [36mpolicy[0m=shared
[36mINFO[0m[2025-04-24T11:25:25.817466944Z] loading plugin "io.containerd.differ.v1.walking"...  [36mtype[0m=io.containerd.differ.v1
[36mINFO[0m[2025-04-24T11:25:25.817492371Z] loading plugin "io.containerd.event.v1.exchange"...  [36mtype[0m=io.containerd.event.v1
[36mINFO[0m[2025-04-24T11:25:25.817507855Z] loading plugin "io.containerd.gc.v1.scheduler"...  [36mtype[0m=io.containerd.gc.v1
[36mINFO[0m[2025-04-24T11:25:25.817833545Z] loading plugin "io.containerd.service.v1.introspection-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.817871512Z] loading plugin "io.containerd.service.v1.containers-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.817897198Z] loading plugin "io.containerd.service.v1.content-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.817930192Z] loading plugin "io.containerd.service.v1.diff-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.817950530Z] loading plugin "io.containerd.service.v1.images-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.817967287Z] loading plugin "io.containerd.service.v1.leases-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.817984515Z] loading plugin "io.containerd.service.v1.namespaces-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.818003468Z] loading plugin "io.containerd.service.v1.snapshots-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.818020790Z] loading plugin "io.containerd.runtime.v1.linux"...  [36mtype[0m=io.containerd.runtime.v1
[36mINFO[0m[2025-04-24T11:25:25.818184575Z] loading plugin "io.containerd.runtime.v2.task"...  [36mtype[0m=io.containerd.runtime.v2
[36mINFO[0m[2025-04-24T11:25:25.818325911Z] loading plugin "io.containerd.monitor.v1.cgroups"...  [36mtype[0m=io.containerd.monitor.v1
[36mINFO[0m[2025-04-24T11:25:25.819324803Z] loading plugin "io.containerd.service.v1.tasks-service"...  [36mtype[0m=io.containerd.service.v1
[36mINFO[0m[2025-04-24T11:25:25.819357469Z] loading plugin "io.containerd.grpc.v1.introspection"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819375059Z] loading plugin "io.containerd.internal.v1.restart"...  [36mtype[0m=io.containerd.internal.v1
[36mINFO[0m[2025-04-24T11:25:25.819596087Z] loading plugin "io.containerd.grpc.v1.containers"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819613865Z] loading plugin "io.containerd.grpc.v1.content"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819628189Z] loading plugin "io.containerd.grpc.v1.diff"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819653753Z] loading plugin "io.containerd.grpc.v1.events"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819670406Z] loading plugin "io.containerd.grpc.v1.healthcheck"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819685377Z] loading plugin "io.containerd.grpc.v1.images"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819700682Z] loading plugin "io.containerd.grpc.v1.leases"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819715379Z] loading plugin "io.containerd.grpc.v1.namespaces"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.819731208Z] loading plugin "io.containerd.internal.v1.opt"...  [36mtype[0m=io.containerd.internal.v1
[36mINFO[0m[2025-04-24T11:25:25.821866795Z] loading plugin "io.containerd.grpc.v1.snapshots"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.821910494Z] loading plugin "io.containerd.grpc.v1.tasks"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.821927777Z] loading plugin "io.containerd.grpc.v1.version"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.821944547Z] loading plugin "io.containerd.tracing.processor.v1.otlp"...  [36mtype[0m=io.containerd.tracing.processor.v1
[36mINFO[0m[2025-04-24T11:25:25.821966130Z] skip loading plugin "io.containerd.tracing.processor.v1.otlp"...  [36merror[0m="no OpenTelemetry endpoint: skip plugin" [36mtype[0m=io.containerd.tracing.processor.v1
[36mINFO[0m[2025-04-24T11:25:25.821981081Z] loading plugin "io.containerd.internal.v1.tracing"...  [36mtype[0m=io.containerd.internal.v1
[31mERRO[0m[2025-04-24T11:25:25.822020256Z] failed to initialize a tracing processor "otlp"  [31merror[0m="no OpenTelemetry endpoint: skip plugin"
[36mINFO[0m[2025-04-24T11:25:25.822059166Z] loading plugin "io.containerd.grpc.v1.cri"...  [36mtype[0m=io.containerd.grpc.v1
[36mINFO[0m[2025-04-24T11:25:25.823580633Z] Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} UntrustedWorkloadRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} Runtimes:map[runc:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[BinaryName: CriuImagePath: CriuPath: CriuWorkPath: IoGid:0 IoUid:0 NoNewKeyring:false NoPivotRoot:false Root: ShimCgroup: SystemdCgroup:false] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:false IgnoreRdtNotEnabledErrors:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate: IPPreference:} Registry:{ConfigPath: Mirrors:map[] Configs:map[] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:registry.k8s.io/pause:3.6 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true DeviceOwnershipFromSecurityContext:false IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false EnableUnprivilegedPorts:false EnableUnprivilegedICMP:false} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri} 
[36mINFO[0m[2025-04-24T11:25:25.823634610Z] Connect containerd service                   
[36mINFO[0m[2025-04-24T11:25:25.823695881Z] Get image filesystem path "/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs" 
[31mERRO[0m[2025-04-24T11:25:25.824626194Z] failed to load cni during init, please check CRI plugin status before setting up network for pods  [31merror[0m="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
[36mINFO[0m[2025-04-24T11:25:25.825230066Z] serving...                                    [36maddress[0m=/run/containerd/containerd.sock.ttrpc
[36mINFO[0m[2025-04-24T11:25:25.825301125Z] serving...                                    [36maddress[0m=/run/containerd/containerd.sock
[36mINFO[0m[2025-04-24T11:25:25.825323225Z] containerd successfully booted in 0.073952s  
[36mINFO[0m[2025-04-24T11:25:25.825348170Z] Start subscribing containerd event           
[36mINFO[0m[2025-04-24T11:25:25.825394872Z] Start recovering state                       
[36mINFO[0m[2025-04-24T11:25:25.825471633Z] Start event monitor                          
[36mINFO[0m[2025-04-24T11:25:25.825491851Z] Start snapshots syncer                       
[36mINFO[0m[2025-04-24T11:25:25.825503169Z] Start cni network conf syncer for default    
[36mINFO[0m[2025-04-24T11:25:25.825515871Z] Start streaming server                       
+ kubeadm config print join-defaults
+ containerd --version
+ export 'CONTAINERD_VERSION=containerd github.com/containerd/containerd v1.6.20 2806fc1057397dbaeefbea0e4e17bddfbd388f38'
+ env_configuration_modify.py
2025-04-24 11:25:26,053 - __main__ - INFO - Begin to config containerd config file /etc/containerd/config.toml
2025-04-24 11:25:26,055 - __main__ - DEBUG - {
    "disabled_plugins": [],
    "imports": [],
    "oom_score": 0,
    "plugin_dir": "",
    "required_plugins": [],
    "root": "/var/lib/containerd",
    "state": "/run/containerd",
    "temp": "",
    "version": 2,
    "cgroup": {
        "path": ""
    },
    "debug": {
        "address": "",
        "format": "",
        "gid": 0,
        "level": "",
        "uid": 0
    },
    "grpc": {
        "address": "/run/containerd/containerd.sock",
        "gid": 0,
        "max_recv_message_size": 16777216,
        "max_send_message_size": 16777216,
        "tcp_address": "",
        "tcp_tls_ca": "",
        "tcp_tls_cert": "",
        "tcp_tls_key": "",
        "uid": 0
    },
    "metrics": {
        "address": "",
        "grpc_histogram": false
    },
    "plugins": {
        "io.containerd.gc.v1.scheduler": {
            "deletion_threshold": 0,
            "mutation_threshold": 100,
            "pause_threshold": 0.02,
            "schedule_delay": "0s",
            "startup_delay": "100ms"
        },
        "io.containerd.grpc.v1.cri": {
            "device_ownership_from_security_context": false,
            "disable_apparmor": false,
            "disable_cgroup": false,
            "disable_hugetlb_controller": true,
            "disable_proc_mount": false,
            "disable_tcp_service": true,
            "enable_selinux": false,
            "enable_tls_streaming": false,
            "enable_unprivileged_icmp": false,
            "enable_unprivileged_ports": false,
            "ignore_image_defined_volumes": false,
            "max_concurrent_downloads": 3,
            "max_container_log_line_size": 16384,
            "netns_mounts_under_state_dir": false,
            "restrict_oom_score_adj": false,
            "sandbox_image": "registry.k8s.io/pause:3.6",
            "selinux_category_range": 1024,
            "stats_collect_period": 10,
            "stream_idle_timeout": "4h0m0s",
            "stream_server_address": "127.0.0.1",
            "stream_server_port": "0",
            "systemd_cgroup": false,
            "tolerate_missing_hugetlb_controller": true,
            "unset_seccomp_profile": "",
            "cni": {
                "bin_dir": "/opt/cni/bin",
                "conf_dir": "/etc/cni/net.d",
                "conf_template": "",
                "ip_pref": "",
                "max_conf_num": 1
            },
            "containerd": {
                "default_runtime_name": "runc",
                "disable_snapshot_annotations": true,
                "discard_unpacked_layers": false,
                "ignore_rdt_not_enabled_errors": false,
                "no_pivot": false,
                "snapshotter": "overlayfs",
                "default_runtime": {
                    "base_runtime_spec": "",
                    "cni_conf_dir": "",
                    "cni_max_conf_num": 0,
                    "container_annotations": [],
                    "pod_annotations": [],
                    "privileged_without_host_devices": false,
                    "runtime_engine": "",
                    "runtime_path": "",
                    "runtime_root": "",
                    "runtime_type": "",
                    "options": {}
                },
                "runtimes": {
                    "runc": {
                        "base_runtime_spec": "",
                        "cni_conf_dir": "",
                        "cni_max_conf_num": 0,
                        "container_annotations": [],
                        "pod_annotations": [],
                        "privileged_without_host_devices": false,
                        "runtime_engine": "",
                        "runtime_path": "",
                        "runtime_root": "",
                        "runtime_type": "io.containerd.runc.v2",
                        "options": {
                            "BinaryName": "",
                            "CriuImagePath": "",
                            "CriuPath": "",
                            "CriuWorkPath": "",
                            "IoGid": 0,
                            "IoUid": 0,
                            "NoNewKeyring": false,
                            "NoPivotRoot": false,
                            "Root": "",
                            "ShimCgroup": "",
                            "SystemdCgroup": false
                        }
                    }
                },
                "untrusted_workload_runtime": {
                    "base_runtime_spec": "",
                    "cni_conf_dir": "",
                    "cni_max_conf_num": 0,
                    "container_annotations": [],
                    "pod_annotations": [],
                    "privileged_without_host_devices": false,
                    "runtime_engine": "",
                    "runtime_path": "",
                    "runtime_root": "",
                    "runtime_type": "",
                    "options": {}
                }
            },
            "image_decryption": {
                "key_model": "node"
            },
            "registry": {
                "config_path": "",
                "auths": {},
                "configs": {},
                "headers": {},
                "mirrors": {}
            },
            "x509_key_pair_streaming": {
                "tls_cert_file": "",
                "tls_key_file": ""
            }
        },
        "io.containerd.internal.v1.opt": {
            "path": "/opt/containerd"
        },
        "io.containerd.internal.v1.restart": {
            "interval": "10s"
        },
        "io.containerd.internal.v1.tracing": {
            "sampling_ratio": 1.0,
            "service_name": "containerd"
        },
        "io.containerd.metadata.v1.bolt": {
            "content_sharing_policy": "shared"
        },
        "io.containerd.monitor.v1.cgroups": {
            "no_prometheus": false
        },
        "io.containerd.runtime.v1.linux": {
            "no_shim": false,
            "runtime": "runc",
            "runtime_root": "",
            "shim": "containerd-shim",
            "shim_debug": false
        },
        "io.containerd.runtime.v2.task": {
            "platforms": [
                "linux/amd64"
            ],
            "sched_core": false
        },
        "io.containerd.service.v1.diff-service": {
            "default": [
                "walking"
            ]
        },
        "io.containerd.service.v1.tasks-service": {
            "rdt_config_file": ""
        },
        "io.containerd.snapshotter.v1.aufs": {
            "root_path": ""
        },
        "io.containerd.snapshotter.v1.devmapper": {
            "async_remove": false,
            "base_image_size": "",
            "discard_blocks": false,
            "fs_options": "",
            "fs_type": "",
            "pool_name": "",
            "root_path": ""
        },
        "io.containerd.snapshotter.v1.native": {
            "root_path": ""
        },
        "io.containerd.snapshotter.v1.overlayfs": {
            "root_path": "",
            "upperdir_label": false
        },
        "io.containerd.snapshotter.v1.zfs": {
            "root_path": ""
        },
        "io.containerd.tracing.processor.v1.otlp": {
            "endpoint": "",
            "insecure": false,
            "protocol": ""
        }
    },
    "proxy_plugins": {},
    "stream_processors": {
        "io.containerd.ocicrypt.decoder.v1.tar": {
            "accepts": [
                "application/vnd.oci.image.layer.v1.tar+encrypted"
            ],
            "args": [
                "--decryption-keys-path",
                "/etc/containerd/ocicrypt/keys"
            ],
            "env": [
                "OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"
            ],
            "path": "ctd-decoder",
            "returns": "application/vnd.oci.image.layer.v1.tar"
        },
        "io.containerd.ocicrypt.decoder.v1.tar.gzip": {
            "accepts": [
                "application/vnd.oci.image.layer.v1.tar+gzip+encrypted"
            ],
            "args": [
                "--decryption-keys-path",
                "/etc/containerd/ocicrypt/keys"
            ],
            "env": [
                "OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"
            ],
            "path": "ctd-decoder",
            "returns": "application/vnd.oci.image.layer.v1.tar+gzip"
        }
    },
    "timeouts": {
        "io.containerd.timeout.bolt.open": "0s",
        "io.containerd.timeout.shim.cleanup": "5s",
        "io.containerd.timeout.shim.load": "5s",
        "io.containerd.timeout.shim.shutdown": "3s",
        "io.containerd.timeout.task.state": "2s"
    },
    "ttrpc": {
        "address": "",
        "gid": 0,
        "uid": 0
    }
}
2025-04-24 11:25:26,059 - __main__ - INFO - Begin to config kubeadm init config file /kubeadm_install/kubeadm_init.yaml
2025-04-24 11:25:26,064 - __main__ - DEBUG - Will modify kubeadm init config {'apiVersion': 'kubeadm.k8s.io/v1beta4', 'bootstrapTokens': [{'groups': ['system:bootstrappers:kubeadm:default-node-token'], 'token': 'abcdef.0123456789abcdef', 'ttl': '24h0m0s', 'usages': ['signing', 'authentication']}], 'kind': 'InitConfiguration', 'localAPIEndpoint': {'advertiseAddress': '1.2.3.4', 'bindPort': 6443}, 'nodeRegistration': {'criSocket': 'unix:///var/run/containerd/containerd.sock', 'imagePullPolicy': 'IfNotPresent', 'imagePullSerial': True, 'name': 'node', 'taints': None}, 'timeouts': {'controlPlaneComponentHealthCheck': '4m0s', 'discovery': '5m0s', 'etcdAPICall': '2m0s', 'kubeletHealthCheck': '4m0s', 'kubernetesAPICall': '1m0s', 'tlsBootstrap': '5m0s', 'upgradeManifests': '5m0s'}}
2025-04-24 11:25:26,064 - __main__ - DEBUG - Will modify kubeadm init config {'apiServer': {}, 'apiVersion': 'kubeadm.k8s.io/v1beta4', 'caCertificateValidityPeriod': '87600h0m0s', 'certificateValidityPeriod': '8760h0m0s', 'certificatesDir': '/etc/kubernetes/pki', 'clusterName': 'kubernetes', 'controllerManager': {}, 'dns': {}, 'encryptionAlgorithm': 'RSA-2048', 'etcd': {'local': {'dataDir': '/var/lib/etcd'}}, 'imageRepository': 'registry.k8s.io', 'kind': 'ClusterConfiguration', 'kubernetesVersion': '1.31.0', 'networking': {'dnsDomain': 'cluster.local', 'serviceSubnet': '10.96.0.0/12'}, 'proxy': {}, 'scheduler': {}}
2025-04-24 11:25:26,064 - __main__ - DEBUG - Will modify kubeadm init config {'kind': 'KubeletConfiguration'}
2025-04-24 11:25:26,064 - __main__ - DEBUG - Will modify kubeadm init config {'kind': 'KubeProxyConfiguration'}
2025-04-24 11:25:26,065 - __main__ - DEBUG - Will dump [{'apiVersion': 'kubeadm.k8s.io/v1beta4', 'bootstrapTokens': [{'groups': ['system:bootstrappers:kubeadm:default-node-token'], 'token': 'abcdef.0123456789abcdef', 'ttl': '24h0m0s', 'usages': ['signing', 'authentication']}], 'kind': 'InitConfiguration', 'localAPIEndpoint': {'advertiseAddress': '0.0.0.0', 'bindPort': 6443}, 'nodeRegistration': {'criSocket': 'unix:///run/containerd/containerd.sock', 'imagePullPolicy': 'IfNotPresent', 'imagePullSerial': True, 'name': 'f194988d7057', 'taints': None}, 'timeouts': {'controlPlaneComponentHealthCheck': '4m0s', 'discovery': '5m0s', 'etcdAPICall': '2m0s', 'kubeletHealthCheck': '4m0s', 'kubernetesAPICall': '1m0s', 'tlsBootstrap': '5m0s', 'upgradeManifests': '5m0s'}, 'provider-id': 'k8s-in-dind://containerd/k8s-v1.31.7-cluster/k8s-v1.31.7-cluster-control-plane', 'skipPhases': ['preflight']}, {'apiServer': {'certSANs': ['k8stest.dev.safedog.site', 'f194988d7057'], 'extraArgs': [{'name': 'authorization-mode', 'value': 'Node,RBAC'}, {'name': 'enable-aggregator-routing', 'value': 'true'}]}, 'apiVersion': 'kubeadm.k8s.io/v1beta4', 'caCertificateValidityPeriod': '87600h0m0s', 'certificateValidityPeriod': '8760h0m0s', 'certificatesDir': '/etc/kubernetes/pki', 'clusterName': 'kubernetes', 'controllerManager': {'enable-hostpath-provisioner': 'true'}, 'dns': {}, 'encryptionAlgorithm': 'RSA-2048', 'etcd': {'local': {'dataDir': '/var/lib/etcd'}}, 'imageRepository': 'registry.aliyuncs.com/google_containers', 'kind': 'ClusterConfiguration', 'kubernetesVersion': 'v1.31.7', 'networking': {'dnsDomain': 'cluster.local', 'serviceSubnet': '10.97.0.0/16', 'podSubnet': '10.245.0.0/16'}, 'proxy': {}, 'scheduler': {}}, {'kind': 'KubeletConfiguration', 'address': '0.0.0.0', 'apiVersion': 'kubelet.config.k8s.io/v1beta1', 'failSwapOn': False, 'cgroupDriver': 'cgroupfs', 'imageGCHighThresholdPercent': 95, 'imageGCLowThresholdPercent': 60, 'evictionHard': {'imagefs.available': '10%', 'memory.available': '200Mi', 'nodefs.available': '10%', 'nodefs.inodesFree': '5%'}, 'evictionPressureTransitionPeriod': '5m0s'}, {'kind': 'KubeProxyConfiguration', 'apiVersion': 'kubeproxy.config.k8s.io/v1alpha1', 'conntrack': {'maxPerCore': 0}, 'iptables': {'minSyncPeriod': '1s'}, 'mode': 'iptables'}] to /kubeadm_install/kubeadm_init.yaml
+ fsync -d /etc/containerd/config.toml
+ fsync -d /kubeadm_install/kubeadm_init.yaml
+ fsync -d /kubeadm_install/kubeadm_join.yaml
+ '[' 0 -ne 0 ]
+ cat /etc/containerd/config.toml
disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
path = ""

[debug]
address = ""
format = ""
gid = 0
level = ""
uid = 0

[grpc]
address = "/run/containerd/containerd.sock"
gid = 0
max_recv_message_size = 16777216
max_send_message_size = 16777216
tcp_address = ""
tcp_tls_ca = ""
tcp_tls_cert = ""
tcp_tls_key = ""
uid = 0

[metrics]
address = ""
grpc_histogram = false

[plugins."io.containerd.gc.v1.scheduler"]
deletion_threshold = 0
mutation_threshold = 100
pause_threshold = 0.02
schedule_delay = "0s"
startup_delay = "100ms"

[plugins."io.containerd.grpc.v1.cri"]
device_ownership_from_security_context = false
disable_apparmor = false
disable_cgroup = false
disable_hugetlb_controller = true
disable_proc_mount = false
disable_tcp_service = true
enable_selinux = false
enable_tls_streaming = false
enable_unprivileged_icmp = false
enable_unprivileged_ports = false
ignore_image_defined_volumes = false
max_concurrent_downloads = 3
max_container_log_line_size = 16384
netns_mounts_under_state_dir = false
restrict_oom_score_adj = false
sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9"
selinux_category_range = 1024
stats_collect_period = 10
stream_idle_timeout = "4h0m0s"
stream_server_address = "127.0.0.1"
stream_server_port = "0"
systemd_cgroup = false
tolerate_missing_hugetlb_controller = true
unset_seccomp_profile = ""

[plugins."io.containerd.grpc.v1.cri".cni]
bin_dir = "/opt/cni/bin"
conf_dir = "/etc/cni/net.d"
conf_template = ""
ip_pref = ""
max_conf_num = 1

[plugins."io.containerd.grpc.v1.cri".containerd]
default_runtime_name = "runc"
disable_snapshot_annotations = true
discard_unpacked_layers = false
ignore_rdt_not_enabled_errors = false
no_pivot = false
snapshotter = "overlayfs"

[plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
base_runtime_spec = ""
cni_conf_dir = ""
cni_max_conf_num = 0
container_annotations = []
pod_annotations = []
privileged_without_host_devices = false
runtime_engine = ""
runtime_path = ""
runtime_root = ""
runtime_type = ""

[plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
base_runtime_spec = ""
cni_conf_dir = ""
cni_max_conf_num = 0
container_annotations = []
pod_annotations = []
privileged_without_host_devices = false
runtime_engine = ""
runtime_path = ""
runtime_root = ""
runtime_type = "io.containerd.runc.v2"

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
BinaryName = ""
CriuImagePath = ""
CriuPath = ""
CriuWorkPath = ""
IoGid = 0
IoUid = 0
NoNewKeyring = false
NoPivotRoot = false
Root = ""
ShimCgroup = ""
SystemdCgroup = false

[plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
base_runtime_spec = ""
cni_conf_dir = ""
cni_max_conf_num = 0
container_annotations = []
pod_annotations = []
privileged_without_host_devices = false
runtime_engine = ""
runtime_path = ""
runtime_root = ""
runtime_type = ""

[plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

[plugins."io.containerd.grpc.v1.cri".image_decryption]
key_model = "node"

[plugins."io.containerd.grpc.v1.cri".registry]
config_path = ""

[plugins."io.containerd.grpc.v1.cri".registry.auths]

[plugins."io.containerd.grpc.v1.cri".registry.configs]

[plugins."io.containerd.grpc.v1.cri".registry.headers]

[plugins."io.containerd.grpc.v1.cri".registry.mirrors]

[plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
tls_cert_file = ""
tls_key_file = ""

[plugins."io.containerd.internal.v1.opt"]
path = "/opt/containerd"

[plugins."io.containerd.internal.v1.restart"]
interval = "10s"

[plugins."io.containerd.internal.v1.tracing"]
sampling_ratio = 1.0
service_name = "containerd"

[plugins."io.containerd.metadata.v1.bolt"]
content_sharing_policy = "shared"

[plugins."io.containerd.monitor.v1.cgroups"]
no_prometheus = false

[plugins."io.containerd.runtime.v1.linux"]
no_shim = false
runtime = "runc"
runtime_root = ""
shim = "containerd-shim"
shim_debug = false

[plugins."io.containerd.runtime.v2.task"]
platforms = [
    "linux/amd64",
]
sched_core = false

[plugins."io.containerd.service.v1.diff-service"]
default = [
    "walking",
]

[plugins."io.containerd.service.v1.tasks-service"]
rdt_config_file = ""

[plugins."io.containerd.snapshotter.v1.aufs"]
root_path = ""

[plugins."io.containerd.snapshotter.v1.devmapper"]
async_remove = false
base_image_size = ""
discard_blocks = false
fs_options = ""
fs_type = ""
pool_name = ""
root_path = ""

[plugins."io.containerd.snapshotter.v1.native"]
root_path = ""

[plugins."io.containerd.snapshotter.v1.overlayfs"]
root_path = ""
upperdir_label = false

[plugins."io.containerd.snapshotter.v1.zfs"]
root_path = ""

[plugins."io.containerd.tracing.processor.v1.otlp"]
endpoint = ""
insecure = false
protocol = ""

[proxy_plugins]

[stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
accepts = [
    "application/vnd.oci.image.layer.v1.tar+encrypted",
]
args = [
    "--decryption-keys-path",
    "/etc/containerd/ocicrypt/keys",
]
env = [
    "OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf",
]
path = "ctd-decoder"
returns = "application/vnd.oci.image.layer.v1.tar"

[stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
accepts = [
    "application/vnd.oci.image.layer.v1.tar+gzip+encrypted",
]
args = [
    "--decryption-keys-path",
    "/etc/containerd/ocicrypt/keys",
]
env = [
    "OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf",
]
path = "ctd-decoder"
returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
"io.containerd.timeout.bolt.open" = "0s"
"io.containerd.timeout.shim.cleanup" = "5s"
"io.containerd.timeout.shim.load" = "5s"
"io.containerd.timeout.shim.shutdown" = "3s"
"io.containerd.timeout.task.state" = "2s"

[ttrpc]
address = ""
gid = 0
uid = 0
+ cat /kubeadm_install/kubeadm_init.yaml
apiVersion: kubeadm.k8s.io/v1beta4
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 0.0.0.0
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  imagePullSerial: true
  name: f194988d7057
  taints: null
provider-id: k8s-in-dind://containerd/k8s-v1.31.7-cluster/k8s-v1.31.7-cluster-control-plane
skipPhases:
- preflight
timeouts:
  controlPlaneComponentHealthCheck: 4m0s
  discovery: 5m0s
  etcdAPICall: 2m0s
  kubeletHealthCheck: 4m0s
  kubernetesAPICall: 1m0s
  tlsBootstrap: 5m0s
  upgradeManifests: 5m0s
---
apiServer:
  certSANs:
  - k8stest.dev.safedog.site
  - f194988d7057
  extraArgs:
  - name: authorization-mode
    value: Node,RBAC
  - name: enable-aggregator-routing
    value: 'true'
apiVersion: kubeadm.k8s.io/v1beta4
caCertificateValidityPeriod: 87600h0m0s
certificateValidityPeriod: 8760h0m0s
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager:
  enable-hostpath-provisioner: 'true'
dns: {}
encryptionAlgorithm: RSA-2048
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.31.7
networking:
  dnsDomain: cluster.local
  podSubnet: 10.245.0.0/16
  serviceSubnet: 10.97.0.0/16
proxy: {}
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: cgroupfs
evictionHard:
  imagefs.available: 10%
  memory.available: 200Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
imageGCHighThresholdPercent: 95
imageGCLowThresholdPercent: 60
kind: KubeletConfiguration
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
conntrack:
  maxPerCore: 0
iptables:
  minSyncPeriod: 1s
kind: KubeProxyConfiguration
mode: iptables
+ '[' -e /run/containerd/containerd.sock ]
+ + '[' -e /var/run/docker.sockdockerd-entrypoint.sh ]

+ echo 'Waiting for dockerd and containerd to start...'
Waiting for dockerd and containerd to start...
+ sleep 3
+ set -eu
+ : true
+ '[' true '=' true ]
+ containerd-entrypoint.sh
+ '[' 0 -eq 0 ]
+ id -u
+ set -eu
+ CONTAINER_LOG_LEVEL=info
+ '[' 0 -eq 0 ]
+ containerdConfig=/etc/containerd/config.toml
+ set -- containerd --config /etc/containerd/config.toml --log-level info
+ exec containerd --config /etc/containerd/config.toml --log-level info
+ uid=0
+ '[' 0 '=' 0 ]
+ dockerSocket=unix:///var/run/docker.sock
+ '[' true '=' true ]
+ '[' -e /run/containerd/containerd.sock ]
+ set -- dockerd '--host=unix:///var/run/docker.sock' --containerd /run/containerd/containerd.sock
+ '[' dockerd '=' dockerd ]
+ find /run /var/run -iname 'docker*.pid' -delete
+ dockerd --version
+ grep -qF ' 20.10.'
[36mINFO[0m[2025-04-24T11:25:26.116400968Z] starting containerd                           [36mrevision[0m=2806fc1057397dbaeefbea0e4e17bddfbd388f38 [36mversion[0m=v1.6.20
[36mINFO[0m[2025-04-24T11:25:26.132533573Z] loading plugin "io.containerd.content.v1.content"...  [36mtype[0m=io.containerd.content.v1
[36mINFO[0m[2025-04-24T11:25:26.132577050Z] loading plugin "io.containerd.snapshotter.v1.aufs"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:26.162610446Z] skip loading plugin "io.containerd.snapshotter.v1.aufs"...  [36merror[0m="aufs is not supported (modprobe aufs failed: exit status 1 \"ip: can't find device 'aufs'\\nmodprobe: module aufs not found in modules.dep\\n\"): skip plugin" [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:26.162686296Z] loading plugin "io.containerd.snapshotter.v1.devmapper"...  [36mtype[0m=io.containerd.snapshotter.v1
[33mWARN[0m[2025-04-24T11:25:26.162730406Z] failed to load plugin io.containerd.snapshotter.v1.devmapper  [33merror[0m="devmapper not configured"
[36mINFO[0m[2025-04-24T11:25:26.162750072Z] loading plugin "io.containerd.snapshotter.v1.native"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:26.162802566Z] loading plugin "io.containerd.snapshotter.v1.overlayfs"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:26.162919679Z] loading plugin "io.containerd.snapshotter.v1.zfs"...  [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:26.163047509Z] skip loading plugin "io.containerd.snapshotter.v1.zfs"...  [36merror[0m="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" [36mtype[0m=io.containerd.snapshotter.v1
[36mINFO[0m[2025-04-24T11:25:26.163063977Z] loading plugin "io.containerd.metadata.v1.bolt"...  [36mtype[0m=io.containerd.metadata.v1
[33mWARN[0m[2025-04-24T11:25:26.163090486Z] could not use snapshotter devmapper in metadata plugin  [33merror[0m="devmapper not configured"
[36mINFO[0m[2025-04-24T11:25:26.163101275Z] metadata content store policy set             [36mpolicy[0m=shared
+ iptables -nL
+ id -u
+ uid=0
+ '[' 0 '!=' 0 ]
+ '[' -x /usr/local/bin/dind ]
+ set -- /usr/local/bin/dind dockerd '--host=unix:///var/run/docker.sock' --containerd /run/containerd/containerd.sock
+ exec /usr/local/bin/dind dockerd '--host=unix:///var/run/docker.sock' --containerd /run/containerd/containerd.sock
[36mINFO[0m[2025-04-24T11:25:26.347717560Z] Starting up                                  
[33mWARN[0m[2025-04-24T11:25:26.351184264Z] could not change group /var/run/docker.sock to docker: group docker not found 
[36mINFO[0m[2025-04-24T11:25:26.387901607Z] Loading containers: start.                   
[36mINFO[0m[2025-04-24T11:25:26.543967760Z] Loading containers: done.                    
[36mINFO[0m[2025-04-24T11:25:26.586308649Z] Docker daemon                                 [36mcommit[0m=94d3ad6 [36mgraphdriver[0m=overlay2 [36mversion[0m=23.0.5
[36mINFO[0m[2025-04-24T11:25:26.586467792Z] Daemon has completed initialization          
[36mINFO[0m[2025-04-24T11:25:26.626848561Z] API listen on /var/run/docker.sock           
+ '[' -e /run/containerd/containerd.sock ]
+ '[' -e /var/run/docker.sock ]
+ exec kubelet-entrypoint.sh
+ k8s-cluster-check.sh
Waiting for kubernetes config files...
I0424 11:25:29.152121     237 initconfiguration.go:261] loading configuration from "/kubeadm_install/kubeadm_init.yaml"
W0424 11:25:29.153091     237 initconfiguration.go:332] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta4", Kind:"ClusterConfiguration"}: strict decoding error: unknown field "controllerManager.enable-hostpath-provisioner"
W0424 11:25:29.153543     237 initconfiguration.go:332] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta4", Kind:"InitConfiguration"}: strict decoding error: unknown field "provider-id"
I0424 11:25:29.154919     237 interface.go:432] Looking for default routes with IPv4 addresses
I0424 11:25:29.154936     237 interface.go:437] Default route transits interface "eth0"
I0424 11:25:29.155319     237 interface.go:209] Interface eth0 is up
I0424 11:25:29.155372     237 interface.go:257] Interface "eth0" has 1 addresses :[172.17.0.2/16].
I0424 11:25:29.155385     237 interface.go:224] Checking addr  172.17.0.2/16.
I0424 11:25:29.155399     237 interface.go:231] IP found 172.17.0.2
I0424 11:25:29.155418     237 interface.go:263] Found valid IPv4 address 172.17.0.2 for interface "eth0".
I0424 11:25:29.155431     237 interface.go:443] Found active IP 172.17.0.2 
I0424 11:25:29.155472     237 common.go:149] WARNING: tolerating control plane version v1.31.7 as a pre-release version
[init] Using Kubernetes version: v1.31.7
[certs] Using certificateDir folder "/etc/kubernetes/pki"
I0424 11:25:29.155766     237 certs.go:112] creating a new certificate authority for ca
[certs] Generating "ca" certificate and key
I0424 11:25:29.199038     237 certs.go:473] validating certificate period for ca certificate
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [f194988d7057 k8stest.dev.safedog.site kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.97.0.1 172.17.0.2]
[certs] Generating "apiserver-kubelet-client" certificate and key
I0424 11:25:29.377629     237 certs.go:112] creating a new certificate authority for front-proxy-ca
[certs] Generating "front-proxy-ca" certificate and key
I0424 11:25:29.773817     237 certs.go:473] validating certificate period for front-proxy-ca certificate
[certs] Generating "front-proxy-client" certificate and key
I0424 11:25:30.031443     237 certs.go:112] creating a new certificate authority for etcd-ca
[certs] Generating "etcd/ca" certificate and key
I0424 11:25:30.161657     237 certs.go:473] validating certificate period for etcd/ca certificate
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [f194988d7057 localhost] and IPs [172.17.0.2 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [f194988d7057 localhost] and IPs [172.17.0.2 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
I0424 11:25:30.722721     237 certs.go:78] creating new public/private key files for signing service account users
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0424 11:25:30.866388     237 kubeconfig.go:111] creating kubeconfig file for admin.conf
[kubeconfig] Writing "admin.conf" kubeconfig file
I0424 11:25:31.107038     237 kubeconfig.go:111] creating kubeconfig file for super-admin.conf
[kubeconfig] Writing "super-admin.conf" kubeconfig file
I0424 11:25:31.381475     237 kubeconfig.go:111] creating kubeconfig file for kubelet.conf
[kubeconfig] Writing "kubelet.conf" kubeconfig file
I0424 11:25:31.596100     237 kubeconfig.go:111] creating kubeconfig file for controller-manager.conf
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0424 11:25:31.709263     237 kubeconfig.go:111] creating kubeconfig file for scheduler.conf
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0424 11:25:31.768533     237 local.go:65] [etcd] wrote Static Pod manifest for a local etcd member to "/etc/kubernetes/manifests/etcd.yaml"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
I0424 11:25:31.768577     237 manifests.go:103] [control-plane] getting StaticPodSpecs
I0424 11:25:31.771362     237 certs.go:473] validating certificate period for CA certificate
I0424 11:25:31.771407     237 manifests.go:129] [control-plane] adding volume "ca-certs" for component "kube-apiserver"
I0424 11:25:31.771412     237 manifests.go:129] [control-plane] adding volume "etc-ca-certificates" for component "kube-apiserver"
I0424 11:25:31.771416     237 manifests.go:129] [control-plane] adding volume "k8s-certs" for component "kube-apiserver"
I0424 11:25:31.771419     237 manifests.go:129] [control-plane] adding volume "usr-local-share-ca-certificates" for component "kube-apiserver"
I0424 11:25:31.771423     237 manifests.go:129] [control-plane] adding volume "usr-share-ca-certificates" for component "kube-apiserver"
I0424 11:25:31.772167     237 manifests.go:158] [control-plane] wrote static Pod manifest for component "kube-apiserver" to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
I0424 11:25:31.772183     237 manifests.go:103] [control-plane] getting StaticPodSpecs
I0424 11:25:31.772419     237 manifests.go:129] [control-plane] adding volume "ca-certs" for component "kube-controller-manager"
I0424 11:25:31.772429     237 manifests.go:129] [control-plane] adding volume "etc-ca-certificates" for component "kube-controller-manager"
I0424 11:25:31.772437     237 manifests.go:129] [control-plane] adding volume "flexvolume-dir" for component "kube-controller-manager"
I0424 11:25:31.772442     237 manifests.go:129] [control-plane] adding volume "k8s-certs" for component "kube-controller-manager"
I0424 11:25:31.772448     237 manifests.go:129] [control-plane] adding volume "kubeconfig" for component "kube-controller-manager"
I0424 11:25:31.772452     237 manifests.go:129] [control-plane] adding volume "usr-local-share-ca-certificates" for component "kube-controller-manager"
I0424 11:25:31.772459     237 manifests.go:129] [control-plane] adding volume "usr-share-ca-certificates" for component "kube-controller-manager"
I0424 11:25:31.773179     237 manifests.go:158] [control-plane] wrote static Pod manifest for component "kube-controller-manager" to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[control-plane] Creating static Pod manifest for "kube-scheduler"
I0424 11:25:31.773190     237 manifests.go:103] [control-plane] getting StaticPodSpecs
I0424 11:25:31.773391     237 manifests.go:129] [control-plane] adding volume "kubeconfig" for component "kube-scheduler"
I0424 11:25:31.773844     237 manifests.go:158] [control-plane] wrote static Pod manifest for component "kube-scheduler" to "/etc/kubernetes/manifests/kube-scheduler.yaml"
I0424 11:25:31.773853     237 kubelet.go:68] Stopping the kubelet
[kubelet-start] No supported init system detected, won't make sure the kubelet not running for a short period of time while setting up configuration for it.
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
W0424 11:25:31.774525     237 kubelet_unix.go:42] cannot determine if systemd-resolved is active: no supported init system detected, skipping checking for services
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[kubelet-start] No supported init system detected, won't make sure the kubelet is running properly.
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0424 11:25:32.814469     247 server.go:491] "Kubelet version" kubeletVersion="v1.31.7-dirty"
I0424 11:25:32.814534     247 server.go:493] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0424 11:25:32.815022     247 server.go:934] "Client rotation is on, will bootstrap in background"
E0424 11:25:32.831671     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
I0424 11:25:32.832196     247 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
E0424 11:25:32.843957     247 log.go:32] "RuntimeConfig from runtime service failed" err="rpc error: code = Unimplemented desc = unknown method RuntimeConfig for service runtime.v1.RuntimeService"
I0424 11:25:32.843992     247 server.go:1408] "CRI implementation should be updated to support RuntimeConfig when KubeletCgroupDriverFromCRI feature gate has been enabled. Falling back to using cgroupDriver from kubelet config."
W0424 11:25:32.851888     247 info.go:53] Couldn't collect info from any of the files in "/etc/machine-id,/var/lib/dbus/machine-id"
I0424 11:25:32.852156     247 server.go:749] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0424 11:25:32.852199     247 server.go:817] "NoSwap is set due to memorySwapBehavior not specified" memorySwapBehavior="" FailSwapOn=false
I0424 11:25:32.853271     247 swap_util.go:113] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1                               partition	999420		999184		-2
 >
I0424 11:25:32.853525     247 container_manager_linux.go:264] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0424 11:25:32.853547     247 container_manager_linux.go:269] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"f194988d7057","RuntimeCgroupsName":"/k8s/system.slice","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"cgroupfs","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"nodefs.inodesFree","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.1},"GracePeriod":0,"MinReclaim":null},{"Signal":"memory.available","Operator":"LessThan","Value":{"Quantity":"200Mi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.1},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null,"CgroupVersion":2}
I0424 11:25:32.853778     247 topology_manager.go:138] "Creating topology manager with none policy"
I0424 11:25:32.853793     247 container_manager_linux.go:300] "Creating device plugin manager"
I0424 11:25:32.854149     247 state_mem.go:36] "Initialized new in-memory state store"
I0424 11:25:32.855849     247 kubelet.go:408] "Attempting to sync node with API server"
I0424 11:25:32.855866     247 kubelet.go:303] "Adding static pod path" path="/etc/kubernetes/manifests"
I0424 11:25:32.855943     247 kubelet.go:314] "Adding apiserver pod source"
I0424 11:25:32.856232     247 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
W0424 11:25:32.858201     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:32.858262     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
I0424 11:25:32.858302     247 kuberuntime_manager.go:262] "Container runtime initialized" containerRuntime="containerd" version="v1.6.20" apiVersion="v1"
W0424 11:25:32.858268     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:32.858354     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
I0424 11:25:32.859793     247 kubelet.go:837] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
W0424 11:25:32.860724     247 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0424 11:25:32.863953     247 server.go:1274] "Started kubelet"
I0424 11:25:32.864133     247 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
I0424 11:25:32.865035     247 server.go:236] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0424 11:25:32.866375     247 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
E0424 11:25:32.868652     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
I0424 11:25:32.869396     247 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0424 11:25:32.869449     247 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-files::/var/lib/kubelet/pki/kubelet.crt::/var/lib/kubelet/pki/kubelet.key"
E0424 11:25:32.869568     247 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"f194988d7057\" not found"
I0424 11:25:32.869598     247 volume_manager.go:289] "Starting Kubelet Volume Manager"
I0424 11:25:32.869719     247 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
I0424 11:25:32.869790     247 reconciler.go:26] "Reconciler: start to sync state"
I0424 11:25:32.869897     247 server.go:449] "Adding debug handlers to kubelet server"
W0424 11:25:32.870105     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:32.870165     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:32.870990     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="200ms"
E0424 11:25:32.871990     247 kubelet.go:1478] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0424 11:25:32.872087     247 factory.go:221] Registration of the systemd container factory successfully
I0424 11:25:32.872197     247 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
I0424 11:25:32.876738     247 factory.go:221] Registration of the containerd container factory successfully
I0424 11:25:32.881509     247 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0424 11:25:32.881525     247 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0424 11:25:32.881544     247 state_mem.go:36] "Initialized new in-memory state store"
I0424 11:25:32.882268     247 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0424 11:25:32.883462     247 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0424 11:25:32.883567     247 status_manager.go:217] "Starting to sync pod status with apiserver"
I0424 11:25:32.883675     247 kubelet.go:2321] "Starting kubelet main sync loop"
E0424 11:25:32.883720     247 kubelet.go:2345] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
W0424 11:25:32.884566     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:32.885681     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
I0424 11:25:32.886467     247 policy_none.go:49] "None policy: Start"
I0424 11:25:32.887186     247 memory_manager.go:170] "Starting memorymanager" policy="None"
I0424 11:25:32.887216     247 state_mem.go:35] "Initializing new in-memory state store"
I0424 11:25:32.890802     247 manager.go:513] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0424 11:25:32.891030     247 eviction_manager.go:189] "Eviction manager: starting control loop"
I0424 11:25:32.891051     247 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
I0424 11:25:32.891445     247 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
E0424 11:25:32.894956     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
I0424 11:25:32.993243     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:32.994801     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:25:33.071662     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="400ms"
I0424 11:25:33.171420     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-etc-ca-certificates\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.171562     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-k8s-certs\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.171731     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-usr-share-ca-certificates\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.171863     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/8f7c1393db6358b232ac19eec10123a1-kubeconfig\") pod \"kube-scheduler-f194988d7057\" (UID: \"8f7c1393db6358b232ac19eec10123a1\") " pod="kube-system/kube-scheduler-f194988d7057"
I0424 11:25:33.172048     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/b39dace32383ff8e4e18de7fea61e1e1-etcd-certs\") pod \"etcd-f194988d7057\" (UID: \"b39dace32383ff8e4e18de7fea61e1e1\") " pod="kube-system/etcd-f194988d7057"
I0424 11:25:33.172261     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-usr-local-share-ca-certificates\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.172411     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/b39dace32383ff8e4e18de7fea61e1e1-etcd-data\") pod \"etcd-f194988d7057\" (UID: \"b39dace32383ff8e4e18de7fea61e1e1\") " pod="kube-system/etcd-f194988d7057"
I0424 11:25:33.172542     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/e3f673805f77221dbc858ffe78fa05b3-ca-certs\") pod \"kube-apiserver-f194988d7057\" (UID: \"e3f673805f77221dbc858ffe78fa05b3\") " pod="kube-system/kube-apiserver-f194988d7057"
I0424 11:25:33.172734     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/e3f673805f77221dbc858ffe78fa05b3-etc-ca-certificates\") pod \"kube-apiserver-f194988d7057\" (UID: \"e3f673805f77221dbc858ffe78fa05b3\") " pod="kube-system/kube-apiserver-f194988d7057"
I0424 11:25:33.172896     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-kubeconfig\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.173031     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/e3f673805f77221dbc858ffe78fa05b3-usr-share-ca-certificates\") pod \"kube-apiserver-f194988d7057\" (UID: \"e3f673805f77221dbc858ffe78fa05b3\") " pod="kube-system/kube-apiserver-f194988d7057"
I0424 11:25:33.173151     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-ca-certs\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.173301     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/36e25f3293fd73a6fa480b415a2e135c-flexvolume-dir\") pod \"kube-controller-manager-f194988d7057\" (UID: \"36e25f3293fd73a6fa480b415a2e135c\") " pod="kube-system/kube-controller-manager-f194988d7057"
I0424 11:25:33.173598     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/e3f673805f77221dbc858ffe78fa05b3-k8s-certs\") pod \"kube-apiserver-f194988d7057\" (UID: \"e3f673805f77221dbc858ffe78fa05b3\") " pod="kube-system/kube-apiserver-f194988d7057"
I0424 11:25:33.173812     247 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/e3f673805f77221dbc858ffe78fa05b3-usr-local-share-ca-certificates\") pod \"kube-apiserver-f194988d7057\" (UID: \"e3f673805f77221dbc858ffe78fa05b3\") " pod="kube-system/kube-apiserver-f194988d7057"
I0424 11:25:33.197445     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:33.198061     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[kubelet-check] The kubelet is healthy after 1.500883048s
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[36mINFO[0m[2025-04-24T11:25:33.294202517Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:25:33.299046270Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:25:33.301388970Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:25:33.302989211Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} 
E0424 11:25:33.472506     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="800ms"
I0424 11:25:33.600684     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:33.601717     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:25:33.793181     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:33.793380     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:33.926545     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:33.926844     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:33.942707     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:33.942980     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:34.274087     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="1.6s"
W0424 11:25:34.385055     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:34.385220     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
I0424 11:25:34.404705     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:34.405622     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:25:35.011130     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:35.876009     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="3.2s"
I0424 11:25:36.008865     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:36.009783     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[33mWARN[0m[2025-04-24T11:25:36.163896388Z] waiting for response from boltdb open         [33mplugin[0m=bolt
W0424 11:25:36.190832     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:36.190974     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:36.624514     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:36.624627     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:36.906928     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:36.907077     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:37.256907     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:37.257050     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:39.040853     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:39.077520     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="6.4s"
I0424 11:25:39.213088     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:39.213631     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:25:40.457247     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:40.457382     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:41.337232     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
W0424 11:25:42.181564     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:42.181709     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:42.401706     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:42.401758     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:42.779984     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:42.780033     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:42.895855     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:25:45.479429     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:25:45.615419     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:45.615826     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:25:47.564712     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:48.979474     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:48.979713     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:51.339491     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
W0424 11:25:51.943955     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:51.944045     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:52.481209     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:25:52.618662     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:52.619566     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:25:52.896128     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
W0424 11:25:53.307278     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:53.307426     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:25:54.541906     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:25:54.542079     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:25:59.483273     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:25:59.622198     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:25:59.623096     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:26:01.341568     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:26:02.897206     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:26:04.298299     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:26:04.299689     247 certificate_manager.go:440] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Reached backoff limit, still unable to rotate certs: timed out waiting for the condition" logger="UnhandledError"
[36mINFO[0m[2025-04-24T11:26:04.410314669Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:26:04.414305801Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.415078     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.415291     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:26:04.415425     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:26:04.415742     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-scheduler-f194988d7057" podUID="8f7c1393db6358b232ac19eec10123a1"
[36mINFO[0m[2025-04-24T11:26:04.420328741Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[36mINFO[0m[2025-04-24T11:26:04.420757879Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[36mINFO[0m[2025-04-24T11:26:04.420892889Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:26:04.422494816Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.423272     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.423470     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:26:04.423597     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:26:04.423882     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\\\": rpc error: code = Unknown desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/etcd-f194988d7057" podUID="b39dace32383ff8e4e18de7fea61e1e1"
[31mERRO[0m[2025-04-24T11:26:04.424689702Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.425299     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.426069     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:26:04.426215     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:26:04.426451     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\\\": rpc error: code = Unknown desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-controller-manager-f194988d7057" podUID="36e25f3293fd73a6fa480b415a2e135c"
[31mERRO[0m[2025-04-24T11:26:04.430163841Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.431277     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:04.431582     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:26:04.432112     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:26:04.432533     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-apiserver-f194988d7057" podUID="e3f673805f77221dbc858ffe78fa05b3"
E0424 11:26:06.484981     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:06.625945     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:06.626873     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:26:08.130277     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:08.130473     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:26:10.690318     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:10.690486     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:26:11.342971     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
W0424 11:26:11.725031     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:11.725230     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:26:12.897488     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:26:13.486835     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:13.629934     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:13.630911     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:26:13.724537     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:13.724739     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
[36mINFO[0m[2025-04-24T11:26:16.888059360Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:26:16.888381619Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:26:18.888359005Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:26:19.888090323Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} 
E0424 11:26:20.487948     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:20.632694     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:20.633084     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:26:21.344984     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:26:22.898684     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:26:27.490140     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:27.635438     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:27.636334     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:26:31.346499     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:26:32.899957     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:26:34.492272     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:34.639434     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:34.640562     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:26:36.304076     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:26:40.516275     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:40.516478     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:26:41.347839     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:26:41.493253     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:41.643496     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:41.644477     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:26:42.901177     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
W0424 11:26:45.116336     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:45.116602     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:26:46.643899     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:26:46.644081     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:26:48.494827     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:48.647285     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:48.648191     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[36mINFO[0m[2025-04-24T11:26:48.749904606Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[36mINFO[0m[2025-04-24T11:26:48.750067356Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:26:48.752141333Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:26:48.752992     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:26:48.753186     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:26:48.753318     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:26:48.753534     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 173.194.203.82:443: i/o timeout\"" pod="kube-system/kube-controller-manager-f194988d7057" podUID="36e25f3293fd73a6fa480b415a2e135c"
[31mERRO[0m[2025-04-24T11:26:48.754403237Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:26:48.755030     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:26:48.755177     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:26:48.755283     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:26:48.755776     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 173.194.203.82:443: i/o timeout\"" pod="kube-system/kube-scheduler-f194988d7057" podUID="8f7c1393db6358b232ac19eec10123a1"
[36mINFO[0m[2025-04-24T11:26:50.694202454Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:26:50.697773540Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:50.698357     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:50.698544     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:26:50.698707     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:26:50.698925     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\\\": rpc error: code = Unknown desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/etcd-f194988d7057" podUID="b39dace32383ff8e4e18de7fea61e1e1"
[36mINFO[0m[2025-04-24T11:26:50.700698495Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:26:50.702701844Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:50.703249     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:26:50.703432     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:26:50.703577     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:26:50.703812     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-apiserver-f194988d7057" podUID="e3f673805f77221dbc858ffe78fa05b3"
E0424 11:26:51.349423     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:26:52.901868     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:26:55.496387     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:26:55.649501     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:26:55.650098     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:27:00.728474     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:27:00.728677     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
[36mINFO[0m[2025-04-24T11:27:00.887540163Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} 
E0424 11:27:01.351394     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:27:02.498007     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:02.652273     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:02.653223     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[36mINFO[0m[2025-04-24T11:27:02.888321828Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:27:02.888738685Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} 
E0424 11:27:02.902322     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
[36mINFO[0m[2025-04-24T11:27:05.887542514Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} 
E0424 11:27:08.303892     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:27:09.499923     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:09.656243     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:09.657174     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:27:11.353102     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:27:12.902692     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:27:16.501908     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:16.658495     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:16.659003     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:27:16.742544     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:27:16.742770     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:27:21.354494     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:27:21.354840     247 event.go:307] "Unable to write event (retry limit exceeded!)" event="&Event{ObjectMeta:{f194988d7057.18393d7be1f428b0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,LastTimestamp:2025-04-24 11:25:32.8639244 +0000 UTC m=+0.699761822,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:27:21.355872     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:27:22.903868     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:27:23.503123     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:23.663610     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:23.664735     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:27:25.082912     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
W0424 11:27:26.101750     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:27:26.101938     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:27:30.505291     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:30.667711     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:30.668601     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[36mINFO[0m[2025-04-24T11:27:31.866622944Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:27:31.870333597Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:27:31.870943     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:27:31.871133     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:27:31.871241     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:27:31.871409     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-controller-manager-f194988d7057" podUID="36e25f3293fd73a6fa480b415a2e135c"
W0424 11:27:32.703904     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:27:32.704069     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
W0424 11:27:32.837801     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:27:32.837942     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:27:32.904876     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
[36mINFO[0m[2025-04-24T11:27:34.726600344Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 74.125.142.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:27:34.728956982Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 74.125.142.82:443: i/o timeout"
E0424 11:27:34.729617     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 74.125.142.82:443: i/o timeout"
E0424 11:27:34.729864     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 74.125.142.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:27:34.729989     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 74.125.142.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:27:34.730208     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 74.125.142.82:443: i/o timeout\"" pod="kube-system/kube-scheduler-f194988d7057" podUID="8f7c1393db6358b232ac19eec10123a1"
E0424 11:27:35.084721     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
[36mINFO[0m[2025-04-24T11:27:35.876991210Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:27:35.880752244Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:27:35.881360     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:27:35.881521     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:27:35.881628     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:27:35.881854     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/etcd-f194988d7057" podUID="b39dace32383ff8e4e18de7fea61e1e1"
E0424 11:27:37.506938     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:37.671465     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:37.672505     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:27:40.303686     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
[36mINFO[0m[2025-04-24T11:27:41.388121094Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:27:41.392378563Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:27:41.393087     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:27:41.393327     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:27:41.393472     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:27:41.393757     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 173.194.203.82:443: i/o timeout\"" pod="kube-system/kube-apiserver-f194988d7057" podUID="e3f673805f77221dbc858ffe78fa05b3"
E0424 11:27:42.905324     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
[36mINFO[0m[2025-04-24T11:27:43.885312834Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} 
E0424 11:27:44.508538     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:44.674984     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:44.675796     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:27:45.086499     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
[36mINFO[0m[2025-04-24T11:27:49.888178577Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:27:50.887939050Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} 
E0424 11:27:51.510241     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:51.678914     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:51.679826     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[36mINFO[0m[2025-04-24T11:27:52.887194004Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} 
E0424 11:27:52.906244     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:27:55.087820     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:27:58.512036     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:27:58.681891     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:27:58.682850     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:27:59.840070     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:27:59.840288     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.17.0.2:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:28:02.907007     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
W0424 11:28:02.997608     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:28:02.997788     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:28:05.089935     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:28:05.513022     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:28:05.685573     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:28:05.686476     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:28:06.725166     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:28:06.725346     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.17.0.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Df194988d7057&limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:28:12.303544     247 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://172.17.0.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:28:12.513632     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:28:12.688865     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:28:12.689833     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:28:12.907528     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:28:15.091812     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
[36mINFO[0m[2025-04-24T11:28:16.914564806Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:28:16.916602085Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:28:16.917200     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:28:16.917368     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:28:16.917473     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-controller-manager-f194988d7057"
E0424 11:28:16.917711     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-controller-manager-f194988d7057_kube-system(36e25f3293fd73a6fa480b415a2e135c)\\\": rpc error: code = Unknown desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-controller-manager-f194988d7057" podUID="36e25f3293fd73a6fa480b415a2e135c"
E0424 11:28:19.515624     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:28:19.692003     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:28:19.692857     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[36mINFO[0m[2025-04-24T11:28:22.752574816Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:28:22.756175886Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:28:22.756774     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:28:22.756957     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:28:22.757074     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/etcd-f194988d7057"
E0424 11:28:22.757283     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"etcd-f194988d7057_kube-system(b39dace32383ff8e4e18de7fea61e1e1)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/etcd-f194988d7057" podUID="b39dace32383ff8e4e18de7fea61e1e1"
E0424 11:28:22.907783     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
W0424 11:28:23.162758     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:28:23.162905     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://172.17.0.2:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
[36mINFO[0m[2025-04-24T11:28:23.185375453Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:28:23.189003983Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:28:23.189442     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout"
E0424 11:28:23.189580     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:28:23.189693     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 142.251.8.82:443: i/o timeout" pod="kube-system/kube-scheduler-f194988d7057"
E0424 11:28:23.189898     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-scheduler-f194988d7057_kube-system(8f7c1393db6358b232ac19eec10123a1)\\\": rpc error: code = Unknown desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 142.251.8.82:443: i/o timeout\"" pod="kube-system/kube-scheduler-f194988d7057" podUID="8f7c1393db6358b232ac19eec10123a1"
[36mINFO[0m[2025-04-24T11:28:24.702069918Z] trying next host                              [36merror[0m="failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" [36mhost[0m=registry.k8s.io
[31mERRO[0m[2025-04-24T11:28:24.705799451Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} failed, error  [31merror[0m="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:28:24.706174     247 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout"
E0424 11:28:24.706267     247 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:28:24.706327     247 kuberuntime_manager.go:1170] "CreatePodSandbox for pod failed" err="rpc error: code = DeadlineExceeded desc = failed to get sandbox image \"registry.k8s.io/pause:3.6\": failed to pull image \"registry.k8s.io/pause:3.6\": failed to pull and unpack image \"registry.k8s.io/pause:3.6\": failed to resolve reference \"registry.k8s.io/pause:3.6\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\": dial tcp 173.194.203.82:443: i/o timeout" pod="kube-system/kube-apiserver-f194988d7057"
E0424 11:28:24.706462     247 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-apiserver-f194988d7057_kube-system(e3f673805f77221dbc858ffe78fa05b3)\\\": rpc error: code = DeadlineExceeded desc = failed to get sandbox image \\\"registry.k8s.io/pause:3.6\\\": failed to pull image \\\"registry.k8s.io/pause:3.6\\\": failed to pull and unpack image \\\"registry.k8s.io/pause:3.6\\\": failed to resolve reference \\\"registry.k8s.io/pause:3.6\\\": failed to do request: Head \\\"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.6\\\": dial tcp 173.194.203.82:443: i/o timeout\"" pod="kube-system/kube-apiserver-f194988d7057" podUID="e3f673805f77221dbc858ffe78fa05b3"
E0424 11:28:25.092909     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
E0424 11:28:26.517745     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:28:26.695459     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:28:26.696333     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
[36mINFO[0m[2025-04-24T11:28:29.884767861Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-controller-manager-f194988d7057,Uid:36e25f3293fd73a6fa480b415a2e135c,Namespace:kube-system,Attempt:0,} 
E0424 11:28:32.908891     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
E0424 11:28:33.519596     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:28:33.698278     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:28:33.699172     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
E0424 11:28:35.094150     247 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.17.0.2:6443/api/v1/namespaces/default/events\": dial tcp 172.17.0.2:6443: connect: connection refused" event="&Event{ObjectMeta:{f194988d7057.18393d7be26f0ac0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:f194988d7057,UID:f194988d7057,APIVersion:,ResourceVersion:,FieldPath:,},Reason:InvalidDiskCapacity,Message:invalid capacity 0 on image filesystem,Source:EventSource{Component:kubelet,Host:f194988d7057,},FirstTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,LastTimestamp:2025-04-24 11:25:32.871977664 +0000 UTC m=+0.707815086,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:f194988d7057,}"
[36mINFO[0m[2025-04-24T11:28:36.888297079Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-f194988d7057,Uid:e3f673805f77221dbc858ffe78fa05b3,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:28:36.888551373Z] RunPodSandbox for &PodSandboxMetadata{Name:kube-scheduler-f194988d7057,Uid:8f7c1393db6358b232ac19eec10123a1,Namespace:kube-system,Attempt:0,} 
[36mINFO[0m[2025-04-24T11:28:36.889092045Z] RunPodSandbox for &PodSandboxMetadata{Name:etcd-f194988d7057,Uid:b39dace32383ff8e4e18de7fea61e1e1,Namespace:kube-system,Attempt:0,} 
E0424 11:28:40.521761     247 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.17.0.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/f194988d7057?timeout=10s\": dial tcp 172.17.0.2:6443: connect: connection refused" interval="7s"
I0424 11:28:40.701888     247 kubelet_node_status.go:72] "Attempting to register node" node="f194988d7057"
E0424 11:28:40.702822     247 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://172.17.0.2:6443/api/v1/nodes\": dial tcp 172.17.0.2:6443: connect: connection refused" node="f194988d7057"
W0424 11:28:42.025551     247 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.17.0.2:6443: connect: connection refused
E0424 11:28:42.026191     247 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.17.0.2:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.17.0.2:6443: connect: connection refused" logger="UnhandledError"
E0424 11:28:42.909874     247 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"f194988d7057\" not found"
